# -*- coding: utf-8 -*-
"""procesar_dataset_productos.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QwXCLWVkQXjldSS8QXtK1n78MIZdN1Mm
"""

#!pip install datasets pandas nltk

import pandas as pd
import re
import nltk
from datasets import load_dataset

# Descargar stopwords de nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
STOPWORDS = set(stopwords.words('english'))

print("Librerías importadas y stopwords descargadas.")

# Expresiones regulares para limpieza
RE_NON_ALPHANUM = re.compile(r'[^A-Za-z0-9.]')
RE_WHITESPACE = re.compile(r'\s+')

def text_cleaning(text: str) -> str:
    """
    Limpia el texto: lo convierte a minúsculas, elimina caracteres no alfanuméricos (excepto el punto)
    y normaliza espacios.
    """
    text = text.strip().lower()
    text = RE_NON_ALPHANUM.sub(' ', text)
    text = RE_WHITESPACE.sub(' ', text)
    return text.strip()

def process_categories(cat):
    """
    Si 'categories' es una lista, une sus elementos aplicando limpieza;
    de lo contrario, limpia la cadena directamente.
    """
    if isinstance(cat, list):
        return ", ".join([text_cleaning(item) for item in cat])
    return text_cleaning(cat)

# Carga el dataset en modo streaming para evitar sobrecargar la RAM
dataset = load_dataset(
    "McAuley-Lab/Amazon-Reviews-2023",
    "raw_meta_Electronics",
    split="full",
    streaming=True,
    trust_remote_code=True
)
print("Dataset cargado en modo streaming.")

import csv

def text_cleaning(text):
    """Limpia y normaliza el texto del título."""
    return text.replace("\n", " ").strip()

def process_categories(categories):
    """Convierte la lista de categorías en un string concatenado por '>'."""
    return " > ".join(categories) if categories else ""

def get_main_image_url(images):
    """
    Retorna la URL principal de la imagen del producto.

    El campo 'images' es un diccionario con claves como 'hi_res', 'large' y 'thumb',
    cada una asociada a una lista de URLs. Se itera en ese orden para devolver la primera
    URL válida (no nula) encontrada.
    """
    if not images or not isinstance(images, dict):
        return ""
    for key in ['hi_res', 'large', 'thumb']:
        urls = images.get(key, [])
        if isinstance(urls, list):
            for url in urls:
                if url:  # Si url es no nula y no vacía
                    return url
    return ""

output_path = "products_clean.csv"
target_records = 150000  # Número de registros deseados
count = 0

with open(output_path, mode="w", encoding="utf-8", newline="") as csvfile:
    fieldnames = ["asin", "title", "categories", "price", "average_rating", "image_url"]
    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
    writer.writeheader()

    for example in dataset:
        # Verificar que existan los campos imprescindibles
        if not (example.get("title") and example.get("categories") and example.get("price") and example.get("average_rating")):
            continue

        # Convertir 'price' a float
        try:
            price = float(example["price"])
        except:
            continue

        # Convertir 'average_rating' a float
        try:
            avg_rating = float(example["average_rating"])
        except:
            continue

        # Procesar título y categorías
        title = text_cleaning(example["title"])
        categories = process_categories(example["categories"])

        # Utilizar 'parent_asin' o 'asin' como identificador único
        asin = example.get("parent_asin", "") or example.get("asin", "")

        # Obtener la URL de la imagen principal, usando la función definida
        image_url = ""
        if example.get("images"):
            image_url = get_main_image_url(example["images"])

        writer.writerow({
            "asin": asin,
            "title": title,
            "categories": categories,
            "price": price,
            "average_rating": avg_rating,
            "image_url": image_url
        })
        count += 1
        if count >= target_records:
            break

print(f"Exportados {count} registros a {output_path}")

#from google.colab import files
#files.download(output_path)